I"–<h1 id="xllama---a-linearzed-xlstm-language-model">xLlaMA - A linearzed xLSTM language model</h1>

<div align="center">

<picture>
  <img alt="specify theme context for images" src="https://hallerpatrick.github.io/assets/images/xllama_logo.svg" width="400px" />
</picture>

Building efficient linear language models


</div>

<h2 id="what-is-xllama">What is xLlaMA?</h2>
<p><b>xLlaMA</b> linearization of the <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B">SmolLM2-1.7B</a> model with <a href="https://arxiv.org/abs/2405.04517">mLSTM</a> as
the token mixer backbone. The model is aligned with 3B token subset of the Fineweb dataset, which we trained with a modified <a href="https://arxiv.org/abs/2408.10189">MOHAWK</a> training scheme.</p>

<h3 id="models">Models</h3>

<p><b>xLlaMA</b> models come in different size, based on the <b>SmolLM2</b> collection.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>HF Model</th>
      <th>Base Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>xLlama-190M</td>
      <td>ðŸ¤— <a href="https://huggingface.co/PatrickHaller/xLlama-190M">xLlama-190M</a></td>
      <td>ðŸ¤— <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M">SmolLM2-135M</a></td>
    </tr>
    <tr>
      <td>xLlama-450M</td>
      <td>ðŸ¤— <a href="https://huggingface.co/PatrickHaller/xLlama-450M">xLlama-450M</a></td>
      <td>ðŸ¤— <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-360M">SmolLM2-360M</a></td>
    </tr>
    <tr>
      <td>xLlama-1.9B</td>
      <td>ðŸ¤— <a href="https://huggingface.co/PatrickHaller/xLlama-1.9B">xLlama-1.9B</a></td>
      <td>ðŸ¤— <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B">SmolLM2-1.7B</a></td>
    </tr>
  </tbody>
</table>

<h2 id="quickstart">Quickstart</h2>

<h3 id="installation">Installation</h3>

<p>For now the model requires a CUDA enabled GPU to run.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">xlstm</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">mlstm_kernels</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">flash</span><span class="o">-</span><span class="n">attn</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">build</span><span class="o">-</span><span class="n">isolation</span>
</code></pre></div></div>

<h3 id="text-generation">Text Generation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s">"PatrickHaller/xLlama-1.9B"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"inference"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s">'text-generation'</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">pipe</span><span class="p">(</span><span class="s">"Once upon a time, there was a"</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="evaluation">Evaluation</h2>

<p>We evaluated each model against common LM benchmarks and also report the recvorage to the original teacher model.</p>

<picture>
  <img alt="specify theme context for images" src="https://hallerpatrick.github.io/assets/images/eval_results_xllama.png" width="800px" />
</picture>

<h2 id="license">License</h2>

<p>Like the SmolLM family, this model is licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 license</a>.</p>

:ET