---
layout: post
clickable: true
layout_type: "Paper"
title: "Empirical Evaluation of Knowledge Distillation from Transformers to
Subquadratic Language Models"
title_prefix: "xLlaMA"
comments: false
description: Paper Publication 
keywords: "NLP, ML, Language Modelling, Linear Attention, xLTM, Linearization"
paper_link: ""

---

<script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p style="color:gray;padding-top:50px"><i class="fa fa-circle" style="color:orange"></i> Chapter 1</p>
# Abstract

